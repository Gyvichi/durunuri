{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP 코드 구조 이해하기 : IMBD 데이터셋""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/CodingVillainKor/DeepDive2TF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\") #(평점, 문자열) 형태\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data_iter): # tokenizer 출력 결과를 generator로\n",
    "    for _, text in data_iter: # IMDB 데이터 형태가 (평점, 문자열)이기에 첫 숫자 형태를 _로 버리고 문자열만 토크나이징.\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_data), specials=[\"<unk>\"])\n",
    "# yield로 얻은 token을 실수 형태로 변환하여 vocab에 저장, unknown token 특수 처리.\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "# 처음 보는 문자열은 0으로."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(imdb, batch_size=8, shuffle=True, collate_fn=collater)\n",
    "# DataLaader : 모델 입력에 용이하게 데이터 변환\n",
    "# collate_fn : batch_size만큼의 데이터들의 label, text, offset끼리 모아 반환해주는 함수\n",
    "\n",
    "def collater(batch): # 합치기. DataLaoder용 함수\n",
    "    labels, texts, offsets = [], [], [0]\n",
    "    for label, text in batch: # label : 평점, text : 문자열\n",
    "        labels.append(label-1) # 평점 1~10 to 0~9. (index로 쓰려고)\n",
    "        text_tensor = torch.tensor(vocab(tokenizer(text)), dtype = torch.long)\n",
    "        # text를 tokenizer로 token화, vocab으로 벡터화, torch.tensor로 텐서화.\n",
    "        texts.append(text_tensor)\n",
    "        # text tensor concat\n",
    "        offsets.append(text_tensor.shape[0])\n",
    "        # offset = 각 index의 sequence 길이\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    texts = torch.cat(texts)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    # .cumsum : \n",
    "    return labels, texts, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embed = nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        # nn.EmbeddingBag는 Batch마다 나누어 임베딩해야 하므로 batch_sz, embed_dim이 출력 차원\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "    \n",
    "    def forward(self, text, offsets, labels=None, train = True):\n",
    "        if train:\n",
    "            loss = self.train_step(text, labels, offsets)\n",
    "            return loss\n",
    "        else:\n",
    "            out = self.predict(text, offsets)\n",
    "            return out\n",
    "\n",
    "# m = Model()\n",
    "# m() \n",
    "# 위 경우 m.__call__() 실행.\n",
    "\n",
    "# 그런데 `nn.Module`을 상속받으면 `m.__call__()`이 `forward`를 호출하여 자동으로 실행됨     \n",
    "# 즉 __init__()이 설계, forward()가 작동부\n",
    "\n",
    "    def train_step(self, text, labels, offsets):\n",
    "        out = self.predict(text, offsets)\n",
    "        loss = self.loss(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, text, offsets):\n",
    "        embed = self.embed(text, offsets)\n",
    "        latent = self.hidden(embed)\n",
    "        out = self.fc(latent)\n",
    "        return out\n",
    "\n",
    "    def loss(self, model_out, label):\n",
    "        loss = F.cross_entropy(model_out, label)\n",
    "        return loss\n",
    "    \n",
    "m = Model(len(vocab), 64, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(m\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# m 실행 -> Model 실행 -> Forward 실행 -> train_step 실행 -> predict 실행.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Users\\xelle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\xelle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\xelle\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 4\u001b[0m, in \u001b[0;36mcollater\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      2\u001b[0m labels, texts, offsets \u001b[38;5;241m=\u001b[39m [], [], [\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label, text \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m----> 4\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m     text_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(vocab(tokenizer(text)), dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# text를 tokenizer로 token화, vocab으로 벡터화, torch.tensor로 텐서화.\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.SGD(m.parameters(), lr = 0.01)\n",
    "#m.parameters() : m.fc.bias, m.weight, m.hidden, m.embed, etc.\n",
    "for e in range(3):\n",
    "    for i, (label, text, offset) in enumerate(dl):\n",
    "        loss = m(text, offset, label) # forward, loss 구하기\n",
    "        # m 실행 -> Model 실행 -> Forward 실행 -> train_step 실행 -> predict 실행.\n",
    "        optim.zero_grad() #optimizer grad 0 초기화\n",
    "        loss.backward() # backward 미분 연산. m.fc.bias.grad 등등 계산\n",
    "        optim.step() # SGD로 update\n",
    "        print(\"Loss : {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = \"It is good and fantastic\"\n",
    "input1 = torch.tensor(vocab(tokenizer(input1)))\n",
    "\n",
    "res = m.predict(review1, offsets = torch.tensor([0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
